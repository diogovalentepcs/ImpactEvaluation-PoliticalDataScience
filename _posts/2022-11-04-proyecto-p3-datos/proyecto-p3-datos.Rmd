---
title: "[Proyecto | Datos] Recopilación y Análisis de Datos"
description: |
  Esta publicación describe los datos y la forma en que fueron recopilados, seguido de un análisis gráfico de las variables de interés.
author:
  - name: Diogo Polónia
    url: https://diogovalentepcs.github.io/website/
date: 2022-11-04 
output:
  distill::distill_article:
    self_contained: false

params:
  datapath: "../../_assets/Data/"
  start_date : 180201
  end_date: 200201
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Recopilación de Datos

Como se discutió en las publicaciones anteriores, trataremos con 3 tipos de datos:

1. Datos de series temporales de concentraciones de CO (variable dependiente) y SO2;
2. Datos de series de tiempo de las condiciones climáticas (velocidad del viento, dirección del viento, humedad relativa y temperatura);
3. Datos de ubicación tanto de las estaciones de monitoreo como de las estaciones de Bike Itaú.

Todos estos datos fueron recolectados manualmente. Del Sistema de Información Nacional de la Calidad del Aire (SINCA), disponible en https://sinca.mma.gob.cl/index.php/region/index/id/M, fue posible recopilar todos los datos sobre la estaciones de monitoreo (todos los datos de series de tiempo). Esta información se recopiló en archivos .csv individuales de 9 de 14 estaciones, ya que solo estas 9 tenían datos para el período de tiempo previsto (2018 - 2020), lo que nos deja un total de 49 archivos, 1 archivo por estación de monitoreo (9 ) por variable (6), a excepción de los datos de SO2, que solo estaban disponibles para el período de tiempo previsto en 4 estaciones ($9 \cdot 5 + 4 = 49$). Estos archivos están disponibles en _assets/Data en el [repositorio de GitHub](https://github.com/diogovalentepcs/politcaldatascience) de este proyecto.

En cuanto a los datos de ubicación, se creó un archivo de Excel independiente con 2 hojas, la primera con los nombres y ubicaciones (en tipo UTM) de las estaciones de monitoreo (recolectadas de SINCA), y la segunda hoja con los ID's, capacidad y coordenadas (latitud y longitud) de cada estación de bicicletas compartidas en Santiago (todas pertenecientes a Bike Itaú). Para esta última información se utilizó Google Maps, lo que me permitió recopilar información de 118 estaciones.

## Preparación de los Datos

La preparación de los datos requiere los siguientes pasos:

1. Filtrado de los conjuntos de datos para el período comprendido entre el 02-01-2018 y el 02-01-2020 (exactamente un año antes y después de la renovación del sistema de Bike Itaú);
2. Fusionar todos los conjuntos de datos en uno solo, con diferentes valores de indicador en diferentes columnas;
3. Validar las conclusiones de artículos anteriores sobre los mejores plazos para analizar la concentración de CO (por ejemplo, las dos horas consecutivas que en promedio marcan las lecturas más altas de CO en la muestra de dos años);
4. Filtre el conjunto de datos para esas horas (7 a. m. a 9 a. m.);
5. Agregue valores para la contaminación de fondo antes de que se forme el pico y considere el promedio de 4 registros de CO consecutivos que, en promedio, mostraron la menor dispersión en el transcurso de la muestra (1 a. m. a 5 a. m.)
6. Construir las variables ficticias que identifican las observaciones posteriores a la implementación, así como el indicador del mes;
7. Pupular con variables para día de la semana, hora del día y mes del año (dummies)

Sobre los datos de ubicación, se requiere:
1. Uniformizar los tipos de datos de ubicación;
2. Calcule las distancias lineales entre cada estación de monitoreo y todas las estaciones de bicicletas, y sume dichas distancias

## EDA (Exploratory Data Analysis) 
Vejamos los datos:

```{r}
library(stringr)
library(dplyr)
library(knitr)

emissions <- c( 'SO2', 'CO')
weather <- c('WindSpeed', 'Temp', 'Hum')
for (item in c(weather, emissions)) {
  data <- read.csv(str_glue('{params$datapath}{item}_PuenteAlto.csv'), header = TRUE, sep = ';')
  show(data[1:5,])
}
```

Limpiar la base CO y filtrar para las fechas que queremos

```{r}
# Rename, drop colunms and change string to numeric
data <- read.csv(str_glue('{params$datapath}CO_PuenteAlto.csv'), header = TRUE, sep = ';')
data <- data %>%
  rename("Date_YYMMDD" = "FECHA..YYMMDD.", 
         "Time_HHMM" = "HORA..HHMM.",
         "CO" = "Registros.validados") %>%
  select(-Registros.preliminares, -Registros.no.validados, -X) %>%
  mutate(CO = as.numeric(gsub(",", ".", CO)))

# Drops unwanted years (before 01 Feb 18 and on or after 01 Feb 2020)
data <- data[data$Date_YYMMDD >= params$start_date & data$Date_YYMMDD < params$end_date, ]

# Veryfy if all days and all hours exist 
if (length(data$Date_YYMMDD) == as.numeric(difftime('2020-01-02', '2018-01-02', units = "days")) * 24) {
  cat("All observations exist! :)")
} else {
  cat("Missing observations... Look out! :(")
}

show(data[1:5,])
```

Juntar todas las bases CO & SO2

```{r}
# Search for CO files
files <- list.files(path=params$datapath, pattern=str_glue("*.csv"), full.names=TRUE, recursive=FALSE)

dataset <- data %>%
  select(-CO)
dataset$index <- 1:nrow(dataset)

# Iterate each file name
bad_dfs <- list()
counter <- 0

for (file in files) {
    
  file_pattern <- sub(str_glue('{params$datapath}/'), '', file)
  file_pattern <- sub('_.*', '', file_pattern)
  
  # Extract station name
  station <- sub(str_glue(".*{file_pattern}_"), "", file)
  station <- sub(".csv.*", "", station)
  col_name <- as.character(str_glue("{file_pattern}_{station}"))
  
  # Create dataframe
  df <- read.csv(file, header = TRUE, sep = ';')
  
  # Clean dataframe
  
  if (file_pattern %in% emissions ) {
      df <- df %>%
             mutate(Registros.validados := as.numeric(gsub(",", ".", Registros.validados))) %>%
             rename("Date_YYMMDD" = "FECHA..YYMMDD.", 
               "Time_HHMM" = "HORA..HHMM.",
                !!col_name := "Registros.validados") %>%
             select(-Registros.preliminares, -Registros.no.validados, -X) 
  } else { 
     df <- df %>%
       mutate(X := as.numeric(gsub(",", ".", X))) %>%
       rename("Date_YYMMDD" = "FECHA..YYMMDD.", 
         "Time_HHMM" = "HORA..HHMM.",
          !!col_name := "X") %>%
       select(-X.1) 
  }
  
  # Drops unwanted years (before 01 Feb 18 and on or after 01 Feb 2020)
  df <- df[df$Date_YYMMDD >= params$start_date & df$Date_YYMMDD < params$end_date, ]

  # Veryfy if all days and all hours exist 
  if (length(df$Date_YYMMDD) != as.numeric(difftime('2020-01-02', '2018-01-02', units = "days")) * 24) {
    bad_dfs[counter] <- station
  }
  
  # Merge df with orginal data
  dataset <- merge(dataset, df, by = c('Date_YYMMDD', 'Time_HHMM'), all = TRUE)
}

if (counter > 0) {
  cat("Some observations don't exist ... Lookout for the following datasets:", bad_dfs)
} else {
  cat("All observations exist! :)")
}

summary(dataset)
```

Datos no incluidos

``` {r}
library(naniar)
vis_miss(dataset %>% select(starts_with('CO')))
vis_miss(dataset %>% select(starts_with('SO2')))
vis_miss(dataset %>% select(starts_with('WindSpeed')))
vis_miss(dataset %>% select(starts_with('Temp')))
vis_miss(dataset %>% select(starts_with('Hum')))

````
Las estaciones de monitoreo de La Florida y Paduhel no tienen datos suficientes para analisis, por lo tanto se eliminará estas estaciones. Para las seguientes, se van usar estrategias de inputación de datos.

````{r}
dataset <- dataset %>% select(-ends_with('LaFlorida'))
dataset <- dataset %>% select(-ends_with('Paduhuel'))
vis_miss(dataset %>% select(starts_with('CO')))
vis_miss(dataset %>% select(starts_with('SO2')))
vis_miss(dataset %>% select(starts_with('WindSpeed')))
vis_miss(dataset %>% select(starts_with('Temp')))
vis_miss(dataset %>% select(starts_with('Hum')))

````
KNN Imputation
...

## Visualizaciones previstas

El principal interés de las visualizaciones es investigar el comportamiento de la variable dependiente (CO), con el fin de validar cómo se construye el modelo. Por lo tanto, el objetivo es crear visualizaciones como estas:

![CO hourly](CO_hourly.png)
![CO monthly](CO_monthly.png)
![CO implementation](CO_implementation.png)

Además, los outliers deben estudiarse a través de boxplots para emisiones y variables climáticas, y, las correlaciones entre estos también deben estudiarse gráficamente.
